{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "densenet 17 02.ipynb",
      "private_outputs": true,
      "provenance": [],
      "history_visible": true,
      "mount_file_id": "1jxRMGJp-CX5bGdeZ4QCRkHsajfEfg5uz",
      "authorship_tag": "ABX9TyN/Ya7reg6kfmJ98r4CR3Z1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/midoo2020/Alzheimer_DL_classification/blob/main/densenet_17_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLsrkXI82hN7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/drive/MyDrive/ColabNotebooks/densnet/train'\n"
      ],
      "metadata": {
        "id": "JEZvlPHz3gQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/drive/MyDrive/ColabNotebooks/densnet/dataset/train'\n",
        "\n",
        "CLASSES = [ 'NonDemented',\n",
        "            'VeryMildDemented',\n",
        "            'MildDemented',\n",
        "            'ModerateDemented']\n",
        "IMG_SIZE = 176\n",
        "IMAGE_SIZE = [176, 176]\n",
        "DIM = (IMG_SIZE, IMG_SIZE)"
      ],
      "metadata": {
        "id": "orndDECD37tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORK_DIR = '/content/drive/MyDrive/ColabNotebooks/densnet/dataset/train'\n",
        "\n",
        "#Performing Image Augmentation to have more data samples\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG\n",
        "ZOOM = [.99, 1.01]\n",
        "BRIGHT_RANGE = [0.8, 1.2]\n",
        "HORZ_FLIP = True\n",
        "FILL_MODE = \"constant\"\n",
        "DATA_FORMAT = \"channels_last\"\n",
        "\n",
        "work_dr = IDG(rescale = 1./255, brightness_range=BRIGHT_RANGE, zoom_range=ZOOM, data_format=DATA_FORMAT, fill_mode=FILL_MODE, horizontal_flip=HORZ_FLIP)\n",
        "\n",
        "train_data_gen = work_dr.flow_from_directory(directory=WORK_DIR, target_size=DIM, batch_size=6400, shuffle=False)\n"
      ],
      "metadata": {
        "id": "-uchOSHQ4Jkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%\n",
        "#Retrieving the data from the ImageDataGenerator iterator\n",
        "\n",
        "train_data, train_labels = train_data_gen.next()\n",
        "\n",
        "# %%\n",
        "#Getting to know the dimensions of our dataset\n",
        "\n",
        "print(train_data.shape, train_labels.shape)\n",
        "\n",
        "# %%pip install imbalanced-learn\n",
        "\n",
        "# %%\n",
        "import imblearn\n",
        "print(imblearn.__version__)\n",
        "\n",
        "# %%\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# %%\n",
        "#Performing over-sampling of the data, since the classes are imbalanced\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "\n",
        "train_data, train_labels = sm.fit_resample(train_data.reshape(-1, IMG_SIZE * IMG_SIZE * 3), train_labels)\n",
        "\n",
        "train_data = train_data.reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "print(train_data.shape, train_labels.shape)\n",
        "\n",
        "# %%\n",
        "#Splitting the data into train, test, and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size = 0.2, random_state=42)\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size = 0.2, random_state=42)\n",
        "\n",
        "\n",
        "# %%\n",
        "from tensorflow.keras.applications.densenet import DenseNet169\n",
        "base_model = DenseNet169(input_shape=(176,176,3),\n",
        "                         include_top=False,\n",
        "                         weights=\"imagenet\")\n",
        "\n",
        "# %%\n",
        "# Freezing Layers\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "# %%\n",
        "# Building Model\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Dense, Activation\n",
        "\n",
        "model=Sequential()\n",
        "model.add(base_model)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Flatten())\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4,activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "# %%\n",
        "from random import randint\n",
        "\n",
        "\n",
        "def show_images(generator,y_pred=None):\n",
        "    \"\"\"\n",
        "    Input: An image generator,predicted labels (optional)\n",
        "    Output: Displays a grid of 9 images with lables\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    # get image lables\n",
        "    labels =dict(zip([0,1,2,3], CLASSES))\n",
        "    \n",
        "    # get a batch of images\n",
        "    x,y = generator.next()\n",
        "\n",
        "    # display a grid of 9 images\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    if y_pred is None:\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            idx = randint(0, 31)\n",
        "            plt.imshow(x[idx])\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(\"Class:{}\".format(labels[np.argmax(y[idx])]))\n",
        "                                                     \n",
        "    else:\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(x[i])\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(\"Actual:{} \\nPredicted:{}\".format(labels[np.argmax(y[i])],labels[y_pred[i]]))\n",
        "    \n",
        "# Display Train Images\n",
        "show_images(train_data_gen)\n",
        "\n",
        "# %%\n",
        "import tensorflow as tf\n",
        "OPT    = tf.keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              metrics=[tf.keras.metrics.AUC(name = 'auc')],\n",
        "              optimizer=OPT)\n",
        "\n",
        "# %%\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# %%\n",
        "#Fit the training data to the model and validate it using the validation data\n",
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=EPOCHS)\n",
        "\n",
        "# %%\n",
        "\n",
        "def Train_Val_Plot(acc,val_acc,loss,val_loss):\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize= (20,15))\n",
        "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
        "\n",
        "    ax1.plot(range(1, len(acc) + 1), acc)\n",
        "    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n",
        "    ax1.set_title('History of Accuracy')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['training', 'validation'])\n",
        "\n",
        "\n",
        "    ax2.plot(range(1, len(loss) + 1), loss)\n",
        "    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n",
        "    ax2.set_title('History of Loss')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['training', 'validation'])\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "Train_Val_Plot(history.history['auc'],history.history['val_auc'],\n",
        "               history.history['loss'],history.history['val_loss'])\n",
        "\n",
        "# %%\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "pred_labels = model.predict(test_data)\n",
        "def roundoff(arr):\n",
        "    \"\"\"To round off according to the argmax of each predicted label array. \"\"\"\n",
        "    arr[np.argwhere(arr != arr.max())] = 0\n",
        "    arr[np.argwhere(arr == arr.max())] = 1\n",
        "    return arr\n",
        "\n",
        "for labels in pred_labels:\n",
        "    labels = roundoff(labels)\n",
        "print(labels)\n",
        "print(classification_report(test_labels, pred_labels, target_names=CLASSES))\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(\"Accuracy is\",roc_auc_score(test_labels, pred_labels))\n",
        "\n",
        "\n",
        "# %%\n",
        "#Plot the confusion matrix to understand the classification in detail\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pred_ls = np.argmax(pred_labels, axis=1)\n",
        "test_ls = np.argmax(test_labels, axis=1)\n",
        "\n",
        "conf_arr = confusion_matrix(test_ls, pred_ls)\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "\n",
        "ax = sns.heatmap(conf_arr, cmap='Greens', annot=True, fmt='d', xticklabels= CLASSES,\n",
        "                yticklabels=CLASSES)\n",
        "plt.savefig('plot.png')\n",
        "plt.title('Alzheimer\\'s Disease Diagnosis')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Truth')\n",
        "plt.show(ax)\n"
      ],
      "metadata": {
        "id": "-M1SrW-j4fXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}